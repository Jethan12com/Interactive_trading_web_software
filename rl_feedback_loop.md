RL Feedback Loop Documentation
Overview
The Reinforcement Learning (RL) feedback loop in CoPilot integrates trade outcomes from JournalEvaluator into the RLTrader to refine the hybrid PPO/DQN models, enhancing signal generation accuracy. This aligns with CoPilotâ€™s mission for modular, scalable, and transparent signal generation.
Components

JournalEvaluator (journal_evaluator.py):
Logs trades with signal_id, pnl, tp_hit, sl_hit, and other metrics.
Updates trade status and triggers RL retraining on trade closure.


RLTrader (reinforcement_trader.py):
Combines PPO (policy-based) and DQN (value-based) models.
Retrains models using historical data and trade outcomes.


DataProvider (data_provider.py):
Fetches historical market data for retraining.


ConfigManager (config_manager.py):
Provides model parameters (e.g., learning rates).



Workflow

Trade Execution:

Signals are generated by SignalEngine and logged via SignalLogger.
Trades are logged in JournalEvaluator with initial OPEN status.


Trade Monitoring:

monitor_trades_sync checks open trades against live data.
Updates pnl, tp_hit, and sl_hit based on stop-loss/take-profit thresholds.
Closes trades (CLOSED status) when TP/SL is hit or manually updated.


Feedback Trigger:

On trade closure, _trigger_rl_feedback is called for RL-based trades (ab_group='A').
Calculates a reward:
Base reward: pnl (profit/loss).
Bonus: +50% if tp_hit=True.
Penalty: -50% if sl_hit=True.


Fetches recent historical data for the pair via DataProvider.


Model Retraining:

RLTrader.train is called with 5,000 timesteps to update PPO/DQN models.
Uses historical data and reward to adjust model weights.
Volatility forecasting (GARCH model) adjusts learning rates dynamically.



Key Metrics

PnL: Drives reward calculation.
TP/SL Hit Rates: Influence reward bonuses/penalties.
Win Rate, Sharpe Ratio: Tracked in get_performance_metrics for model evaluation.

Safety and Scalability

Safety: Feedback only triggers for RL trades; demo mode testing recommended.
Scalability: Modular design allows swapping RL models or adding new data sources.
Logging: Detailed logs in logs/journal_evaluator.log and logs/reinforcement_trader.log.

Future Improvements

Integrate pattern signals from pattern_discovery.py into rewards.
Add adaptive retraining frequency based on performance metrics.
Support multi-pair retraining for portfolio-wide learning.
